{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684f92c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5468\\2224253654.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715ff2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Mapping labels...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print('Loading dataset...')\n",
    "df = pd.read_csv('Twitter_Sentiments.csv', encoding='latin-1', header=None, on_bad_lines='skip')\n",
    "df = df[[0, 5]]\n",
    "df.columns = ['label', 'tweet']\n",
    "\n",
    "# Map: 0 (negative) -> 1, 2 (neutral) -> 0, 4 (positive) -> 2\n",
    "print('Mapping labels...')\n",
    "df['label'] = df['label'].map({0: 1, 2: 0, 4: 2})  # 1=negative, 0=neutral, 2=positive\n",
    "df = df[df['label'].isin([0, 1, 2])]\n",
    "df = df.dropna(subset=['tweet'])\n",
    "df['tweet'] = df['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d9004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slang dictionary\n",
    "slang_dict = {\n",
    "    \"gr8\": \"great\", \"luv\": \"love\", \"b4\": \"before\", \"u\": \"you\", \"ur\": \"your\", \"omg\": \"oh my god\",\n",
    "    \"idk\": \"i do not know\", \"smh\": \"shaking my head\", \"tbh\": \"to be honest\", \"lmao\": \"laughing\",\n",
    "    \"wtf\": \"what the heck\", \"btw\": \"by the way\", \"thx\": \"thanks\", \"pls\": \"please\", \"plz\": \"please\",\n",
    "    \"imo\": \"in my opinion\", \"imho\": \"in my humble opinion\", \"fyi\": \"for your information\", \"brb\": \"be right back\",\n",
    "    \"bff\": \"best friend forever\", \"rofl\": \"rolling on the floor laughing\", \"afaik\": \"as far as i know\",\n",
    "    \"irl\": \"in real life\", \"jk\": \"just kidding\", \"np\": \"no problem\", \"ty\": \"thank you\", \"yw\": \"you are welcome\",\n",
    "    \"gg\": \"good game\", \"ftw\": \"for the win\", \"atm\": \"at the moment\", \"bc\": \"because\", \"cya\": \"see you\",\n",
    "    \"dm\": \"direct message\", \"fb\": \"facebook\", \"fomo\": \"fear of missing out\", \"hmu\": \"hit me up\",\n",
    "    \"icymi\": \"in case you missed it\", \"ily\": \"i love you\", \"lmk\": \"let me know\", \"nvm\": \"never mind\",\n",
    "    \"omw\": \"on my way\", \"tba\": \"to be announced\", \"tbd\": \"to be decided\", \"tgif\": \"thank god it's friday\",\n",
    "    \"ttyl\": \"talk to you later\", \"wyd\": \"what are you doing\", \"ya\": \"you\", \"tho\": \"though\", \"wanna\": \"want to\",\n",
    "    \"gonna\": \"going to\", \"gotta\": \"got to\", \"kinda\": \"kind of\", \"sorta\": \"sort of\", \"ain't\": \"is not\",\n",
    "    \"w8\": \"wait\", \"bday\": \"birthday\", \"cuz\": \"because\", \"coz\": \"because\", \"dunno\": \"do not know\",\n",
    "    \"sup\": \"what is up\", \"yo\": \"hello\", \"fam\": \"family\", \"bae\": \"before anyone else\", \"lit\": \"amazing\",\n",
    "    \"salty\": \"bitter\", \"savage\": \"bold\", \"slay\": \"succeed\", \"fire\": \"excellent\", \"goat\": \"greatest of all time\",\n",
    "    \"noob\": \"newbie\", \"stan\": \"support\", \"tea\": \"gossip\", \"vibe\": \"atmosphere\", \"yeet\": \"throw\", \"sus\": \"suspicious\",\n",
    "    \"cap\": \"lie\", \"bet\": \"okay\", \"flex\": \"show off\", \"ghost\": \"ignore\", \"lowkey\": \"quietly\", \"highkey\": \"openly\",\n",
    "    \"mood\": \"relatable\", \"shade\": \"insult\", \"ship\": \"support relationship\", \"snatched\": \"perfect\", \"thirsty\": \"desperate\",\n",
    "    \"woke\": \"aware\", \"yolo\": \"you only live once\"\n",
    "}\n",
    "def replace_slang(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([slang_dict.get(w.lower(), w) for w in words])\n",
    "df['tweet'] = df['tweet'].apply(replace_slang)\n",
    "\n",
    "def handle_negation(text):\n",
    "    text = re.sub(r'not ([a-zA-Z]+)', r'not_\\1', text)\n",
    "    return text\n",
    "df['tweet'] = df['tweet'].apply(handle_negation)\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for word in r:\n",
    "        input_txt = re.sub(word, \"\", input_txt)\n",
    "    return input_txt\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3fb233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up rule-based sarcasm detection...\n",
      "Detecting sarcasm using rule-based approach...\n"
     ]
    }
   ],
   "source": [
    "# Sarcasm detection (rule-based approach - no model download required)\n",
    "print('Setting up rule-based sarcasm detection...')\n",
    "\n",
    "# Mixed sentiment detection\n",
    "positive_words = set([\n",
    "    \"good\", \"great\", \"excellent\", \"amazing\", \"love\", \"fantastic\", \"awesome\", \"wonderful\", \"superb\", \"outstanding\",\n",
    "    \"brilliant\", \"positive\", \"enjoy\", \"happy\", \"pleased\", \"delight\", \"satisfied\", \"best\", \"favorite\", \"cool\", \"nice\",\n",
    "    \"perfect\", \"impressive\", \"sweet\", \"beautiful\", \"fun\", \"success\", \"win\", \"winning\", \"blessed\", \"grateful\", \"excited\"\n",
    "])\n",
    "negative_words = set([\n",
    "    \"bad\", \"terrible\", \"awful\", \"hate\", \"worst\", \"poor\", \"disappoint\", \"sad\", \"angry\", \"upset\", \"horrible\", \"negative\",\n",
    "    \"unhappy\", \"annoyed\", \"frustrated\", \"fail\", \"failure\", \"problem\", \"issue\", \"sucks\", \"lame\", \"boring\", \"dull\",\n",
    "    \"disgust\", \"regret\", \"pain\", \"annoy\", \"unimpressed\", \"mediocre\", \"crap\", \"garbage\", \"trash\", \"bug\", \"broken\"\n",
    "])\n",
    "# Sarcasm indicators\n",
    "sarcasm_indicators = {\n",
    "    'exaggeration_words': ['literally', 'obviously', 'clearly', 'totally', 'completely', 'absolutely', 'definitely'],\n",
    "    'irony_words': ['sure', 'right', 'yeah', 'okay', 'whatever', 'fine', 'great'],\n",
    "    'sarcasm_phrases': ['oh great', 'wonderful', 'fantastic', 'brilliant', 'genius', 'smart', 'clever'],\n",
    "    'question_marks': ['?', '??', '???'],\n",
    "    'capitalization': ['ALL CAPS', 'MiXeD cAsE'],\n",
    "    'emoticons': [':)', ':-)', ';)', ';-)', ':/', ':-/', ':|', ':-|'],\n",
    "    'hashtags': ['#sarcasm', '#not', '#irony', '#sure', '#whatever']\n",
    "}\n",
    "\n",
    "def detect_sarcasm_rule_based(text):\n",
    "    \"\"\"\n",
    "    Rule-based sarcasm detection using linguistic patterns\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    sarcasm_score = 0\n",
    "    \n",
    "    # Check for exaggeration words\n",
    "    for word in sarcasm_indicators['exaggeration_words']:\n",
    "        if word in text_lower:\n",
    "            sarcasm_score += 1\n",
    "    \n",
    "    # Check for irony words (especially when used with negative context)\n",
    "    irony_count = sum(1 for word in sarcasm_indicators['irony_words'] if word in text_lower)\n",
    "    if irony_count > 0:\n",
    "        # Check if there are negative words nearby\n",
    "        negative_context = any(word in text_lower for word in negative_words)\n",
    "        if negative_context:\n",
    "            sarcasm_score += irony_count * 2\n",
    "    \n",
    "    # Check for sarcasm phrases\n",
    "    for phrase in sarcasm_indicators['sarcasm_phrases']:\n",
    "        if phrase in text_lower:\n",
    "            sarcasm_score += 2\n",
    "    \n",
    "    # Check for excessive punctuation\n",
    "    if text.count('!') > 2 or text.count('?') > 2:\n",
    "        sarcasm_score += 1\n",
    "    \n",
    "    # Check for ALL CAPS\n",
    "    if text.isupper() and len(text) > 5:\n",
    "        sarcasm_score += 2\n",
    "    \n",
    "    # Check for mixed case (sArCaSm)\n",
    "    if any(c.isupper() for c in text[1:]) and any(c.islower() for c in text[1:]):\n",
    "        sarcasm_score += 1\n",
    "    \n",
    "    # Check for emoticons\n",
    "    for emoticon in sarcasm_indicators['emoticons']:\n",
    "        if emoticon in text:\n",
    "            sarcasm_score += 1\n",
    "    \n",
    "    # Check for sarcasm hashtags\n",
    "    for hashtag in sarcasm_indicators['hashtags']:\n",
    "        if hashtag in text_lower:\n",
    "            sarcasm_score += 3\n",
    "    \n",
    "    # Check for contradiction patterns\n",
    "    contradiction_patterns = [\n",
    "        ('good', 'bad'), ('great', 'terrible'), ('love', 'hate'),\n",
    "        ('amazing', 'awful'), ('perfect', 'worst'), ('best', 'worst')\n",
    "    ]\n",
    "    \n",
    "    for pos_word, neg_word in contradiction_patterns:\n",
    "        if pos_word in text_lower and neg_word in text_lower:\n",
    "            sarcasm_score += 2\n",
    "    \n",
    "    # Check for \"not\" + positive word patterns\n",
    "    not_positive_patterns = [\n",
    "        'not good', 'not great', 'not amazing', 'not perfect', 'not love',\n",
    "        'not happy', 'not excited', 'not thrilled'\n",
    "    ]\n",
    "    \n",
    "    for pattern in not_positive_patterns:\n",
    "        if pattern in text_lower:\n",
    "            sarcasm_score += 1\n",
    "    \n",
    "    # Return 1 if sarcasm score is high enough, 0 otherwise\n",
    "    return int(sarcasm_score >= 2)\n",
    "\n",
    "print('Detecting sarcasm using rule-based approach...')\n",
    "df['sarcasm'] = df['tweet'].apply(detect_sarcasm_rule_based)\n",
    "\n",
    "\n",
    "def is_mixed_sentiment(text):\n",
    "    words = set(text.lower().split())\n",
    "    return int(len(words & positive_words) > 0 and len(words & negative_words) > 0)\n",
    "df['mixed_sentiment'] = df['tweet'].apply(is_mixed_sentiment)\n",
    "\n",
    "# Copy this code and add it as a new cell BEFORE the model training cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a20391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text data...\n",
      "Creating Bag of Words features...\n",
      "Adding extra features...\n",
      "Feature matrix shape: (1340179, 5002)\n",
      "Preprocessing completed!\n",
      "Checking label distribution...\n",
      "Unique labels in dataset: [np.int64(0), np.int64(1), np.int64(2)]\n",
      "Label counts: label\n",
      "0     25000\n",
      "1    800000\n",
      "2    515179\n",
      "Name: count, dtype: int64\n",
      "After remapping - Unique labels: [np.int64(0), np.int64(1), np.int64(2)]\n",
      "Label counts: label\n",
      "0     25000\n",
      "1    800000\n",
      "2    515179\n",
      "Name: count, dtype: int64\n",
      "Splitting data...\n",
      "Training XGBoost...\n",
      "Evaluating...\n",
      "F1 Score: 0.7089, Accuracy: 0.7279\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing text data...')\n",
    "\n",
    "# Apply text preprocessing\n",
    "df['tweet'] = df['tweet'].apply(remove_pattern, pattern=\"@[\\w]*\")\n",
    "df['tweet'] = df['tweet'].apply(remove_emojis)\n",
    "df['tweet'] = df['tweet'].apply(lambda x: re.sub(\"[^a-zA-Z#]\", \" \", x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join([w for w in x.split() if len(w) > 3]))\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join([stemmer.stem(w) for w in x.split()]))\n",
    "\n",
    "print('Creating Bag of Words features...')\n",
    "# Create Bag of Words features\n",
    "bow_vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
    "bow_features = bow_vectorizer.fit_transform(df['tweet'])\n",
    "\n",
    "print('Adding extra features...')\n",
    "# Add extra features (sarcasm and mixed sentiment)\n",
    "extra_features = np.column_stack([df['sarcasm'], df['mixed_sentiment']])\n",
    "\n",
    "# Combine BOW features with extra features\n",
    "bow_with_features = hstack([bow_features, extra_features])\n",
    "\n",
    "print(f'Feature matrix shape: {bow_with_features.shape}')\n",
    "print('Preprocessing completed!')\n",
    "print('Checking label distribution...')\n",
    "print(f'Unique labels in dataset: {sorted(df[\"label\"].unique())}')\n",
    "print(f'Label counts: {df[\"label\"].value_counts().sort_index()}')\n",
    "\n",
    "# Fix the label mapping issue\n",
    "if 0 not in df['label'].unique():\n",
    "    print('Remapping labels to [0, 1] for 2-class problem...')\n",
    "    df['label'] = df['label'].map({1: 0, 2: 1})  # 1->0 (negative), 2->1 (neutral)\n",
    "    num_classes = 2\n",
    "else:\n",
    "    num_classes = 3\n",
    "\n",
    "print(f'After remapping - Unique labels: {sorted(df[\"label\"].unique())}')\n",
    "print(f'Label counts: {df[\"label\"].value_counts().sort_index()}')\n",
    "\n",
    "print('Splitting data...')\n",
    "x_train, x_test, y_train, y_test = train_test_split(bow_with_features, df['label'], random_state=42, test_size=0.25)\n",
    "\n",
    "print('Training XGBoost...')\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='mlogloss', \n",
    "    objective='multi:softmax', \n",
    "    num_class=num_classes\n",
    ")\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# Save model and vectorizer\n",
    "pickle.dump(xgb_model, open('regmodel.pkl', 'wb'))\n",
    "joblib.dump(bow_vectorizer, 'bow_vectorizer.joblib')\n",
    "\n",
    "print('Evaluating...')\n",
    "pred = xgb_model.predict(x_test)\n",
    "f1 = f1_score(y_test, pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(f'F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae9ef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking label distribution...\n",
      "Unique labels in dataset: [np.int64(0), np.int64(1), np.int64(2)]\n",
      "Label counts: label\n",
      "0     25000\n",
      "1    800000\n",
      "2    515179\n",
      "Name: count, dtype: int64\n",
      "Setting up 3-class sentiment analysis...\n",
      "Current unique labels: [np.int64(0), np.int64(1), np.int64(2)]\n",
      "Final unique labels: [np.int64(0), np.int64(1), np.int64(2)]\n",
      "Label counts: label\n",
      "0     25000\n",
      "1    800000\n",
      "2    515179\n",
      "Name: count, dtype: int64\n",
      "Splitting data...\n",
      "Training XGBoost...\n",
      "Evaluating...\n",
      "F1 Score: 0.7089, Accuracy: 0.7279\n"
     ]
    }
   ],
   "source": [
    "print('Checking label distribution...')\n",
    "print(f'Unique labels in dataset: {sorted(df[\"label\"].unique())}')\n",
    "print(f'Label counts: {df[\"label\"].value_counts().sort_index()}')\n",
    "\n",
    "# FIXED: Ensure we have 3 classes for proper sentiment analysis\n",
    "print('Setting up 3-class sentiment analysis...')\n",
    "print(f'Current unique labels: {sorted(df[\"label\"].unique())}')\n",
    "\n",
    "# Make sure we have labels 0, 1, 2\n",
    "if 0 not in df['label'].unique():\n",
    "    print('Remapping labels to ensure 3 classes...')\n",
    "    # Map: 1->0 (negative), 2->1 (neutral), and we'll need positive class\n",
    "    df['label'] = df['label'].map({1: 0, 2: 1})  # 1->0 (negative), 2->1 (neutral)\n",
    "    \n",
    "    # If we don't have positive examples, we can create some from neutral\n",
    "    # For now, let's work with what we have and ensure 3 classes\n",
    "    print('Note: Model will be trained with available classes')\n",
    "    \n",
    "num_classes = 3  # Force 3 classes\n",
    "print(f'Final unique labels: {sorted(df[\"label\"].unique())}')\n",
    "print(f'Label counts: {df[\"label\"].value_counts().sort_index()}')\n",
    "\n",
    "\n",
    "\n",
    "print('Splitting data...')\n",
    "x_train, x_test, y_train, y_test = train_test_split(bow_with_features, df['label'], random_state=42, test_size=0.25)\n",
    "\n",
    "print('Training XGBoost...')\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='mlogloss', \n",
    "    objective='multi:softmax', \n",
    "    num_class=num_classes\n",
    ")\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# Save model and vectorizer\n",
    "pickle.dump(xgb_model, open('regmodel.pkl', 'wb'))\n",
    "joblib.dump(bow_vectorizer, 'bow_vectorizer.joblib')\n",
    "\n",
    "print('Evaluating...')\n",
    "pred = xgb_model.predict(x_test)\n",
    "f1 = f1_score(y_test, pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(f'F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eabcc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n",
      "Training XGBoost...\n",
      "Evaluating...\n",
      "F1 Score: 0.7089, Accuracy: 0.7279\n"
     ]
    }
   ],
   "source": [
    "print('Splitting data...')\n",
    "x_train, x_test, y_train, y_test = train_test_split(bow_with_features, df['label'], random_state=42, test_size=0.25)\n",
    "\n",
    "print('Training XGBoost...')\n",
    "# FIXED: Force 3-class classification\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='mlogloss', \n",
    "    objective='multi:softmax', \n",
    "    num_class=3  # Force 3 classes: 0=neutral, 1=negative, 2=positive\n",
    ")\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# Save model and vectorizer\n",
    "pickle.dump(xgb_model, open('regmodel.pkl', 'wb'))\n",
    "joblib.dump(bow_vectorizer, 'bow_vectorizer.joblib')\n",
    "\n",
    "print('Evaluating...')\n",
    "pred = xgb_model.predict(x_test)\n",
    "f1 = f1_score(y_test, pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(f'F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77f212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for prediction!\n",
      "Input: not bad -> Prediction: negative\n",
      "Input: yeah right, that was helpful -> Prediction: negative\n",
      "Input: the food was good but the service was terrible -> Prediction: negative\n",
      "Input: idk if I love or hate this -> Prediction: negative\n",
      "Input: this is lit -> Prediction: positive\n",
      "Input: what a surprise, another bug -> Prediction: negative\n",
      "Input: meh -> Prediction: negative\n",
      "Input: so helpful, thanks a lot -> Prediction: positive\n"
     ]
    }
   ],
   "source": [
    "print('Ready for prediction!')\n",
    "def preprocess_review(review):\n",
    "    review = replace_slang(review)\n",
    "    review = handle_negation(review)\n",
    "    review = remove_pattern(review, \"@[\\w]*\")\n",
    "    review = re.sub(\"[^a-zA-Z#]\", \" \", review)\n",
    "    review = remove_emojis(review)\n",
    "    review = review.encode('ascii', 'ignore').decode('ascii')\n",
    "    review = \" \".join([w for w in review.split() if len(w) > 3])\n",
    "    review = \" \".join([stemmer.stem(w) for w in review.split()])\n",
    "    return review\n",
    "\n",
    "def predict_sentiment(review):\n",
    "    sarcasm = detect_sarcasm_rule_based(review)\n",
    "    mixed = is_mixed_sentiment(review)\n",
    "    processed_review = preprocess_review(review)\n",
    "    review_bow = bow_vectorizer.transform([processed_review])\n",
    "    extra = np.array([[sarcasm, mixed]])\n",
    "    review_with_features = hstack([review_bow, extra])\n",
    "    prediction = xgb_model.predict(review_with_features)[0]\n",
    "    if prediction == 2:\n",
    "        return 'positive'\n",
    "    elif prediction == 1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Example usage\n",
    "test_texts = [\n",
    "    \"not bad\", \"yeah right, that was helpful\", \"the food was good but the service was terrible\",\n",
    "    \"idk if I love or hate this\", \"this is lit\", \"what a surprise, another bug\", \"meh\", \"so helpful, thanks a lot\"\n",
    "]\n",
    "for text in test_texts:\n",
    "    print(f'Input: {text} -> Prediction: {predict_sentiment(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3308ef2-8172-4c96-a338-53c7805ff696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bow_vectorizer.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(xgb_model, open('regmodel.pkl', 'wb'))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(bow_vectorizer, 'bow_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d655b35-cecf-4b9c-bf36-613d2005746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model=pickle.load(open('regmodel.pkl','rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
